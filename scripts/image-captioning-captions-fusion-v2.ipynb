{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9294289,"sourceType":"datasetVersion","datasetId":5627018},{"sourceId":9315052,"sourceType":"datasetVersion","datasetId":5641789},{"sourceId":9503348,"sourceType":"datasetVersion","datasetId":5783886},{"sourceId":9577953,"sourceType":"datasetVersion","datasetId":5839259},{"sourceId":52890,"sourceType":"modelInstanceVersion","modelInstanceId":44362,"modelId":52393},{"sourceId":85986,"sourceType":"modelInstanceVersion","modelInstanceId":72246,"modelId":78150},{"sourceId":110653,"sourceType":"modelInstanceVersion","modelInstanceId":44362,"modelId":52393}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport keras\nimport keras_nlp\nimport numpy as np\nimport PIL\nimport requests\nimport io\nimport matplotlib\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport glob\nfrom tqdm import tqdm\n\n# Set the Keras backend to JAX and configure float precision\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\nkeras.config.set_floatx(\"bfloat16\")","metadata":{"_uuid":"93bb4588-648a-46b2-8caa-3028db862c28","_cell_guid":"f185611a-8c7b-48bb-b809-daa657427238","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-08T20:38:06.404384Z","iopub.execute_input":"2024-10-08T20:38:06.405196Z","iopub.status.idle":"2024-10-08T20:38:22.233208Z","shell.execute_reply.started":"2024-10-08T20:38:06.405150Z","shell.execute_reply":"2024-10-08T20:38:22.232403Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load pre-trained models - Paligemma\npaligemma = keras_nlp.models.PaliGemmaCausalLM.from_preset(\"pali_gemma_3b_mix_224\")","metadata":{"_uuid":"29a36f91-3203-494f-b402-d840ba06ba6a","_cell_guid":"9f3d52fc-14a8-456e-8234-244fbc44eb9c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-08T20:39:53.909038Z","iopub.execute_input":"2024-10-08T20:39:53.910234Z","iopub.status.idle":"2024-10-08T20:41:06.795736Z","shell.execute_reply.started":"2024-10-08T20:39:53.910186Z","shell.execute_reply":"2024-10-08T20:41:06.794927Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load pre-trained models - Gemma 2\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")","metadata":{"_uuid":"e965a3df-fc27-4e1d-a399-372b1a42485e","_cell_guid":"f21ed03e-8b31-4c7a-b1d6-3017c6f35a01","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-08T20:42:04.017095Z","iopub.execute_input":"2024-10-08T20:42:04.017915Z","iopub.status.idle":"2024-10-08T20:43:13.635243Z","shell.execute_reply.started":"2024-10-08T20:42:04.017871Z","shell.execute_reply":"2024-10-08T20:43:13.634173Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define input and output directories for images and captions\ninputDir = '/kaggle/input/test-rendered-imgs/rendered_imgs'\noutputDir = '/kaggle/working/2D_captions_prompt'\n\ndef crop_and_resize(image, target_size):\n    \"\"\"\n    Crops and resizes an image to a specified target size.\n\n    Args:\n        image (PIL.Image): The input image.\n        target_size (tuple): Desired output size (height, width) for the image.\n\n    Returns:\n        PIL.Image: The cropped and resized image.\n    \"\"\"\n    width, height = image.size\n    source_size = min(image.size)\n    left = width // 2 - source_size // 2\n    top = height // 2 - source_size // 2\n    right = left + source_size\n    bottom = top + source_size\n\n    # Crop and resize the image\n    return image.crop((left, top, right, bottom)).resize(target_size)\n\ndef read_image(filepath, target_size=(224, 224)):\n    \"\"\"\n    Reads an image from a local file path and resizes it.\n\n    Args:\n        filepath (str): Path to the image file.\n        target_size (tuple): Desired output size (height, width) for the image.\n\n    Returns:\n        numpy.ndarray: The image data as a NumPy array with shape (height, width, channels).\n    \"\"\"\n    try:\n        image = Image.open(filepath)\n        image = crop_and_resize(image, target_size)\n        image = np.array(image)\n        if image.shape[2] == 4:\n            image = image[:, :, :3]\n        return image\n    except Exception as e:\n        print(f\"Error reading {filepath}: {str(e)}\")\n        return None\n\ndef clean_output_folder(output_folder):\n    \"\"\"\n    Cleans the output folder by removing all files.\n\n    Args:\n        output_folder (str): Path to the folder to be cleaned.\n    \"\"\"\n    files = glob.glob(f\"{output_folder}/*\")\n    for file in files:\n        os.remove(file)\n        print(f\"Deleted {file}\")\n\ndef generate_captions(view_number, target_size=(224, 224)):\n    \"\"\"\n    Generates captions for images in a specific view folder.\n\n    Args:\n        view_number (int): The view number of the images.\n        target_size (tuple): Desired output size (height, width) for the images.\n    \"\"\"\n    outfilename = f'{outputDir}/2D_captions_view{view_number}.txt'\n    infolder = f'{inputDir}/rendered_imgs_view{view_number}/*.png'\n\n    all_files = glob.glob(infolder)\n    all_imgs = [x for x in all_files if x.endswith(\".png\")]\n    print(f\"Number of .png images: {len(all_imgs)}\")\n\n    for filename in tqdm(all_imgs):\n        current_image = read_image(filename, target_size)\n        if current_image is None:\n            continue\n\n        caption_prompt = 'caption en.\\n'\n        output = paligemma.generate(\n            inputs={\n                \"images\": current_image,\n                \"prompts\": caption_prompt,\n            }, max_length = 300\n        )\n        output = output.removeprefix(caption_prompt).strip()\n        print(output)\n\n        image_name = os.path.basename(filename).split('.')[0]\n        outdirectory = outputDir\n        os.makedirs(outdirectory, exist_ok=True)\n\n        with open(outfilename, 'a+') as f:\n            f.write(f\"{image_name} : {output}\\n\")\n\ndef read_captions_files(file_paths):\n    \"\"\"\n    Reads captions from multiple text files and groups them by the base image name.\n\n    Args:\n        file_paths (list): List of paths to the caption files.\n\n    Returns:\n        dict: A dictionary where keys are base image names and values are lists of captions.\n    \"\"\"\n    captions = {}\n    for file_path in file_paths:\n        with open(file_path, 'r') as f:\n            lines = f.readlines()\n            for line in lines:\n                image_name, caption = line.split(\": \", 1)\n                base_image_name = image_name.rsplit('_', 1)[0]\n                if base_image_name not in captions:\n                    captions[base_image_name] = []\n                captions[base_image_name].append(caption.strip())\n    return captions\n\ndef clean_fused_caption(caption, prompt_text, caption_list):\n    \"\"\"\n    Clean up the generated caption by removing unwanted tags, the original prompt, \n    and the image captions from the fused caption.\n    \n    Args:\n        caption (str): The fused caption returned by the model.\n        prompt_text (str): The original prompt used in the caption generation.\n        caption_list (list): The list of original image captions.\n\n    Returns:\n        str: The cleaned fused caption.\n    \"\"\"\n    # Remove the original prompt and image captions\n    caption = caption.replace(prompt_text, \"\")\n\n    # Remove each of the individual captions from the final caption\n    for original_caption in caption_list:\n        caption = caption.replace(original_caption, \"\")\n\n    # Remove unwanted tags\n    caption = caption.strip()\n    caption = caption.replace(\"<start_of_turn>user\", \"\").replace(\"<end_of_turn>\", \"\")\n    caption = caption.replace(\"<start_of_turn>model\", \"\").replace(\"<end_of_turn>\", \"\")\n\n    # Return the cleaned caption\n    return caption.strip()\n\ndef fuse_captions(captions):\n    \"\"\"\n    Fuses multiple captions into a single concise caption for each image.\n\n    Args:\n        captions (dict): A dictionary of image names and their corresponding list of captions.\n\n    Returns:\n        dict: A dictionary of image names and their fused captions.\n    \"\"\"\n    start_of_turn_user = \"<start_of_turn>user\\n\"\n    start_of_turn_model = \"<start_of_turn>model\\n\"\n    end_of_turn = \"<end_of_turn>\\n\"\n\n    fused_captions = {}\n\n    for image_name, caption_list in tqdm(captions.items()):\n        # Create the text to be passed to the model\n        text = \"; \".join(caption_list)\n        prompt = start_of_turn_user + \\\n                 f\"Given a set of descriptions about the same 3D object, distill these descriptions into one coherent caption. Pay attention to all necessary details. Do not provide any explanation. The descriptions are as follows: '{text}'.\" + \\\n                 end_of_turn + start_of_turn_model\n\n        # Generate the fused caption\n        fused_caption = gemma_lm.generate(prompt, max_length=300).strip()\n\n        # Clean the fused caption by removing the prompt and original captions\n        cleaned_caption = clean_fused_caption(fused_caption, prompt, caption_list)\n\n        # Store the cleaned caption\n        fused_captions[image_name] = cleaned_caption\n\n    return fused_captions","metadata":{"_uuid":"55bc3702-11cb-41e4-9035-d769269bfc8d","_cell_guid":"c470eb47-e7b0-4c70-a3e7-1d2b234c7fa6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-08T21:32:13.691015Z","iopub.execute_input":"2024-10-08T21:32:13.691398Z","iopub.status.idle":"2024-10-08T21:32:13.714830Z","shell.execute_reply.started":"2024-10-08T21:32:13.691348Z","shell.execute_reply":"2024-10-08T21:32:13.713784Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clean the output folder before generating new captions\nclean_output_folder(outputDir)\n\n# Generate captions for images in all views\nfor i in range(8):\n    generate_captions(i)","metadata":{"_uuid":"4f607016-93e4-4fb9-99dd-d9715700c7e0","_cell_guid":"37f83fc6-91ae-4970-95fc-e9b27ed6893a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-08T21:32:16.085390Z","iopub.execute_input":"2024-10-08T21:32:16.086271Z","iopub.status.idle":"2024-10-08T21:34:29.077645Z","shell.execute_reply.started":"2024-10-08T21:32:16.086231Z","shell.execute_reply":"2024-10-08T21:34:29.076666Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read captions from the generated files\nfile_paths = [f\"{outputDir}/2D_captions_view{i}.txt\" for i in range(8)]\ncaptions = read_captions_files(file_paths)\n\n# Fuse the captions\nfused_captions = fuse_captions(captions)","metadata":{"_uuid":"0e25d5b6-6e6b-4a11-910f-020c7553aea6","_cell_guid":"ba9f6f08-263c-4aae-b1f9-c5a884fefa4f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-08T21:38:19.147433Z","iopub.execute_input":"2024-10-08T21:38:19.147861Z","iopub.status.idle":"2024-10-08T21:38:30.110437Z","shell.execute_reply.started":"2024-10-08T21:38:19.147820Z","shell.execute_reply":"2024-10-08T21:38:30.109444Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the final fused captions\nfor image_name, fused_caption in fused_captions.items():\n    print(f\"Object: {image_name}\\nFused Caption: {fused_caption}\\n\" + \"-\"*40 + \"\\n\")","metadata":{"_uuid":"bd1e4de3-b759-49d6-a67e-ad047cdf651e","_cell_guid":"c35905a7-1955-4b8c-a2e2-ac1a0345fbd3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-08T21:38:34.912417Z","iopub.execute_input":"2024-10-08T21:38:34.912807Z","iopub.status.idle":"2024-10-08T21:38:34.918509Z","shell.execute_reply.started":"2024-10-08T21:38:34.912769Z","shell.execute_reply":"2024-10-08T21:38:34.917516Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print and save the final fused captions\nwith open('/kaggle/working/3D_captions_prompt\\.txt', 'a+') as f:\n    for object_name, fused_caption in fused_captions.items():\n        print(f\"Object: {object_name}\\nFused Caption: {fused_caption}\\n\" + \"-\"*40 + \"\\n\")\n        f.write(f\"{object_name} : {fused_caption}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-10-08T21:38:46.690610Z","iopub.execute_input":"2024-10-08T21:38:46.691244Z","iopub.status.idle":"2024-10-08T21:38:46.697521Z","shell.execute_reply.started":"2024-10-08T21:38:46.691205Z","shell.execute_reply":"2024-10-08T21:38:46.696484Z"},"trusted":true},"outputs":[],"execution_count":null}]}